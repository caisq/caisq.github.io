<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Shanqing Cai's Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
        <link rel="stylesheet" href="assets/css/scai.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->

        <script type="text/javascript">

            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-70350806-1']);
            _gaq.push(['_trackPageview']);

            (function() {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();

        </script>
  </head>
  
	<body>
		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
                            <h1 id="title">Shanqing Cai, Ph.D.</h1>
                        </div>

					<!-- Nav -->
						<nav id="nav">
							<!--

								Prologue's nav expects links in one of two formats:

								1. Hash link (scrolls to a different section within the page)

								   <li><a href="#foobar" id="foobar-link" class="icon fa-whatever-icon-you-want skel-layers-ignoreHref"><span class="label">Foobar</span></a></li>

								2. Standard link (sends the user to another page/site)

								   <li><a href="http://foobar.tld" id="foobar-link" class="icon fa-whatever-icon-you-want"><span class="label">Foobar</span></a></li>

							-->
							<ul>
                                <li><a href="#top" id="top-link" class="skel-layers-ignoreHref"><span class="icon fa-home">Top</span></a></li>
                                <li><a href="#ml" id="glyphoid-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Machine Learning</span></a></li>
                                <li><a href="#glyphoid" id="glyphoid-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Glyphoid</span></a></li>
                                <li><a href="#audapter" id="audapter-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Audapter</span></a></li>
                                <li><a href="#papers" id="papers-link" class="skel-layers-ignoreHref"><span class="icon fa-th">Papers</span></a></li>
                                <li><a href="#papers_ml" id="papers-ml-link" class="level2Link"><span class="icon fa-th-list">ML</span></a></li>
                                <li><a href="#papers_speech" id="papers-speech-link" class="level2Link"><span class="icon fa-th-list">Speech</span></a></li>
                                <li><a href="#papers_audition" id="papers-audition-link" class="level2Link"><span class="icon fa-th-list">Hearing</span></a></li>
                                <li><a href="#papers_visual_protheses" id="papers-vp-link" class="level2Link"><span class="icon fa-th-list">Bionic eyes</span></a></li>
								<li><a href="#about" id="about-link" class="skel-layers-ignoreHref"><span class="icon fa-user">About</span></a></li>
							</ul>
						</nav>

				</div>

				<div class="bottom">

					<!-- Social Icons -->
                    <ul class="icons">
                        <li><a href="https://www.facebook.com/caisq" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
                        <li><a href="https://github.com/caisq" class="icon fa-github"><span class="label">GitHub</span></a></li>
                        <li><a href="https://github.com/shanqing-cai" class="icon fa-github"><span class="label">GitHub</span></a></li>
                        <li><a href="https://www.linkedin.com/in/shanqingcai" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
                        <li><a href="https://twitter.com/sqcai" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
                        <li><a href="https://caisq.wordpress.com/" class="icon fa-wordpress"><span class="label">LinkedIn</span></a></li>
                    </ul>
				</div>

			</div>

		    <!-- Main -->
			<div id="main">

				<!-- Intro -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
                                <p class="welcomeMessage">Shanqing Cai</p>
                                <p class="auxName">
                                    <div id="aux-name-ipa">[
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Voiceless_retroflex_sibilant">ʂ</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Open_front_unrounded_vowel">a</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Dental,_alveolar_and_postalveolar_nasals">n</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Tone_letter">˥˩</a>&nbsp;&nbsp;
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Voiceless_alveolo-palatal_affricate">t͡ɕ</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Aspirated_consonant">ʰ</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Close_front_unrounded_vowel">i</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Velar_nasal">ŋ</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Tone_letter">˥</a>&nbsp;&nbsp;
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Voiceless_alveolar_affricate">t͡s</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Aspirated_consonant">ʰ</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Diphthong">ai</a>
                                        <a target="_blank" href="https://en.wikipedia.org/wiki/Tone_letter">˥˩</a>
                                        ]
                                    </div>
                                    <div>蔡善清</div>
                                </p>
							</header>
                            <body class="intro-body">
                            </body>

							<footer class="intro-footer">
                                <a href="#ml" class="button nav-button scrolly">
                                    <img style="width:1em;height:1em;vertical-align:middle;margin-right:0.25em"
                                        src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Tensorflow_logo.svg/1000px-Tensorflow_logo.svg.png" />
                                    Machine Learning
                                </a>
                                <br/>
                                <a href="#glyphoid" class="button nav-button scrolly">Glyphoid</a>
                                <!-- <a href="./glyphoid/" class="button glyphoidLinkButton glyphoid-launch" style="color:black;"><img style="width:1.25em;height:1.25em;vertical-align:middle;" src="images/glyphoid_32x32_white_2.png" /></a> -->
                                <br/>
                                <a href="#audapter" class="button nav-button scrolly">Audapter</a>
                                <br/>
                                <a href="#papers" class="button nav-button scrolly">Papers</a>
                                <br/>
                                <a href="#about" class="button nav-button scrolly">About</a>
							</footer>

						</div>
					</section>


                <!-- Intro -->
                <section id="ml" class="two">
                    <div class="container">
                        <header>
                            <h2>Machine Learning</h2>
                            <p>
                                Currently a software engineer at <a target="_blank" href="https://en.wikipedia.org/wiki/Google_Brain">Google Brain</a>,
                                Shanqing is one of the developers of <a target="_blank" href="https://www.tensorflow.org/">TensorFlow</a>,
                                a popular open-source framework for deep learning and artificial intelligence.
                            </p>
                            <p>
                                Among other projects, Shanqing is working on or has worked on:
                                <ul class="unnumberedList">
                                    <li>
                                        <a target="_blank" href="https://www.tensorflow.org/js/">TensorFlow.js</a>:
                                        Bringing the power of deep learning to the browser and the larger JavaScript ecosystem
                                    </li>
                                    <li>
                                      <a  target="_blacnk" href="https://www.manning.com/books/deep-learning-with-javascript">
                                        Deep Learning with JavaScript: Neural Networks in TensorFlow.js:
                                      </a>
                                      A co-authored book that introduces JavaScript and web developers to the world of deep learning and A.I.
                                    </li>
                                    <li>
                                        <a target="_blank" href="https://data-flair.training/blogs/tensorflow-debugging/">TensorFlow Debugger (tfdbg) in TF 1.x</a>:
                                        A debugger for computation graphs in TensorFlow 1.x
                                    </li>
                                    <li>
                                        <a target="_blank" href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/debugger">
                                            TensorBoard Debugger Plugin
                                        </a>: A visual debugger for TensorFlow, implemented as a
                                        <a target="_blank" href="https://github.com/tensorflow/tensorboard-plugin-example/blob/master/README.md">
                                            TensorBoard Plugin
                                        </a> (Version for TF 2.x is under active development)
                                    </li>
                                    <li>
                                        <a target="_blank" href="https://www.tensorflow.org/guide/eager">
                                            Eager execution: 
                                        </a>
                                        TensorFlow's define-by-run API, with focus on usability aspects and code examples, e.g.,
                                        dynamic computation graphs
      
                                    </li>
                                    <li>
                                        Infrastructure for the testing, release and open-sourcing of TensorFlow.
                                    </li>
                                </ul>
                            </p>
                        </header>
                    </div>
                </section>

                <!-- glyphoid -->
                <section id="glyphoid" class="two">
                    <div class="container">

                        <header>
                            <h2>Glyphoid</h2>
                        </header>

                        <p>Glyphoid is an open-source parser and calculator of handwritten mathematical formulae. <br/></p>
                        <p>
                            <a href="https://github.com/Glyphoid" class="button"><img style="width:1.05em;height:1.05em;vertical-align:middle;" src="images/GitHub-Mark-inv-32px.png" /> Source code</a>
                        </p>

                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9LFmDcpyZ0w?list=PLcUSYoM0otQi4qCaO5uzluG8ww69kgepc" frameborder="0" allowfullscreen></iframe>
                        <br/>
                        <a href="https://www.youtube.com/watch?v=SlsEhwm3Whk&list=PLcUSYoM0otQi4qCaO5uzluG8ww69kgepc&index=4" class="button">More demo videos (YouTube) &#128279;</a>

                        <br/><br/>


                    </div>
                </section>

                <!-- audapter -->
                <section id="audapter" class="two">
                    <div class="container">

                        <header>
                            <h2>Audapter</h2>
                        </header>

                        <div>
                            <a class="fancybox" rel="group" href="./images/audapter-schematic.png" style="font-size:75%;">Click to enlarge</a>
                            <br/>
                            <a class="fancybox" rel="group" href="./images/audapter-schematic.png"><img class="imageWithShadow" width="360" height="296" src="./images/audapter-schematic.png" alt="Schematic of Audapter" /></a>
                        </div>

                        <p>
                            "Audapter", previously known as "TransShiftMex", is a <a href="https://www.mathworks.com/products/matlab/index.html?refresh=true">MATLAB-based</a> software package for configurable,
                            real-time manipulation of speech signals. It is designed for research on auditory-motor
                            interaction in speech production, but may also be of interest to certain speech signal
                            processing applications. Audapter is currently capable of perturbing the following acoustic
                            parameters of speech:
                            <ul class="audapterNumberedList">
                                <li>Formant frequencies (F1 and F2), in both static and time-varying ways</li>
                                <li>Fundamental frequency (F0)</li>
                                <li>Local timing, through real-time tracking and time-warping</li>
                                <li>Local intensity</li>
                                <li>Global time delay (delayed auditory feedback)</li>
                                <li>Global intensity</li>
                            </ul>

                            Downloads:
                            <ul class="unnumberedList">
                                <li><a href="http://sites.bu.edu/guentherlab/files/2016/11/AudapterManual.pdf">Manual (PDF)</a></li>
                                <li><a href="http://sites.bu.edu/guentherlab/files/2016/11/Audapter_MEX-Files.zip">MEX files</a></li>
                                <li><a href="https://github.com/shanqing-cai/audapter_matlab"><img style="width:1.05em;height:1.05em;vertical-align:middle;" src="images/GitHub-Mark-32px.png" /> MATLAB scripts</a></li>
                                <li><a href="https://github.com/shanqing-cai/audapter_mex"><img style="width:1.05em;height:1.05em;vertical-align:middle;" src="images/GitHub-Mark-32px.png" /> C++ MEX source code of Audapter</a></li>
                            </ul>

                            Requirements:
                            <ul class="audapterNumberedList">
                                <li>Windows operating system</li>
                                <li><a href="https://www.mathworks.com/products/matlab/index.html?refresh=true">MATLAB</a></li>
                                <li>ASIO-compatible sound cards and ASIO driver - see <a href="AudapterManual.pdf">manual</a> for details</li>
                            </ul>

                        </p>

                    </div>
                </section>



                <!-- Papers -->
                <section id="papers" class="two">

                    <div class="container">

                        <header>
                            <h2>Papers</h2>
                        </header>

                        <body>
                        <p>
                            <a href="http://scholar.google.com/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=Shanqing+Cai&as_publication=&as_ylo=&as_yhi=&as_sdt=1.&as_sdtp=on&as_sdtf=&as_sdts=22&hl=en" class="button sectionExternalLinks">Google Scholar &#128279;</a>
                            <a href="http://www.ncbi.nlm.nih.gov/pubmed?term=shanqing%20cai%20or%20shanquing%20cai" class="button sectionExternalLinks">PubMed &#128279;</a>
                            <a href="http://dblp.uni-trier.de/pers/hc/c/Cai:Shanqing" class="button sectionExternalLinks">DBLP &#128279;</a>
                            <br/>
                            <a href="http://www.researchgate.net/profile/Shanqing_Cai" class="button sectionExternalLinks">ResearchGate &#128279;</a>

                            <br/><br/>
                            <a href="#papers_ml" class="button scrolly papersSubsectionButton">Machine Learning</a>
                            <a href="#papers_speech" class="button scrolly papersSubsectionButton">Speech</a>
                            <a href="#papers_audition" class="button scrolly papersSubsectionButton">Hearing</a>
                            <a href="#papers_visual_protheses" class="button scrolly papersSubsectionButton">Bionic eyes</a>
                        </p>

                        <h3 id="papers_ml">&#167; Machine Learning</h3>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    Smilkov D, Thorat N, Assogba Y, Yuan A, Kreeger N, Yu P, Zhang K,
                                    <u>Cai S</u>, Nielsen E, Soergel D, Bileschi S, Terry M,
                                    Nicholson C, Gupta SN, Sirajuddin S, Sculley D, Monga R, Corrado G, Viegas FB, Wattenberg M.
                                    Tensorflow.js: Machine learning for the web and beyond.
                                    SysML 2019.
                                    <div  style="float:right;">
                                        <a target="_blank" href="https://arxiv.org/abs/1901.05350" class="button paperButton">ArXiv &#128279;</a>
                                    </div>
                                </li>
                                <li class="paperListItem">
                                    <u>Cai S</u>, Breck E, Nielsen E, Salib M, Sculley D. (2016).
                                    TensorFlow Debugger: Debugging Dataflow Graphs for Machine Learning.
                                    Proceedings of the Reliable Machine Learning in the Wild - NIPS 2016 Workshop.
                                    <div  style="float:right;">
                                        <a target="_blank" href="https://research.google.com/pubs/pub45789.html" class="button paperButton">Google Research &#128279;</a>
                                    </div>
                                </li>
                                <li class="paperListItem">
                                    Breck E,, <u>Cai S</u>, Nielsen E, Salib M, Sculley D. (2016).
                                    What’s your ML test score? A rubric for ML production systems.
                                    Proceedings of the Reliable Machine Learning in the Wild - NIPS 2016 Workshop.
                                    <div  style="float:right;">
                                        <a target="_blank" href="https://research.google.com/pubs/pub45742.html" class="button paperButton">Google Research &#128279;</a>
                                    </div>
                                </li>
                            </ul>
                        </p>

                        <h3 id="papers_speech">&#167; Speech production and its brain mechanisms</h3>

                        <p><h4>Journal papers</h4></p>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    Daliri A, Wieland EA, <u>Cai S</u>, Guenther FH, Chang SE. (2017).
                                    Auditory‐motor adaptation is reduced in adults who stutter but not in children who stutter.
                                    Developmental Science.
                                    <div  style="float:right;">
                                        <a href="https://www.ncbi.nlm.nih.gov/pubmed/28256029" class="button paperButton freePaperButton">PMC &#128279;</a>
                                        <a href="http://onlinelibrary.wiley.com/doi/10.1111/desc.12521/abstract" class="button paperButton">Wiley &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Sitek KR, <u>Cai S</u>, Beal DS, Perkell JS, Guenther FH, Ghosh SS. (2016).
                                    Decreased Cerebellar-Orbitofrontal Connectivity Correlates with Stuttering Severity:
                                    Whole-Brain Functional and Structural Connectivity Associations with Persistent Developmental Stuttering
                                    Front. Hum. Neurosci.
                                    <div  style="float:right;">
                                        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4855981/" class="button paperButton freePaperButton">PMC &#128279;</a>
                                        <a href="https://www.frontiersin.org/articles/10.3389/fnhum.2016.00190/full" class="button paperButton freePaperButton">Front. Hum. Neurosci. &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Patel R, Reilly KJ, Archibald E, <u>Cai S</u>, Guenther FH. (2015).
                                    Responses to Intensity-Shifted Auditory Feedback During Running Speech.
                                    J. Speech Lang. Hear. Res. Available online: 2015-10-01
                                    <div  style="float:right;">
                                        <a href="http://jslhr.pubs.asha.org/article.aspx?articleid=2451430" class="button paperButton">JSLHR &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                <u>Cai S</u>, Tourville JA, Beal DS, Perkell JS, Ghosh SS. (2014). Diffusion Imaging of
                                    Cerebral White Matter in Persons Who Stutter: Evidence for Network-Level
                                    Anomalies. Frontiers Hum. Neurosci. 8:54.
                                    <div  style="float:right;">
                                        <a href="http://www.frontiersin.org/Journal/10.3389/fnhum.2014.00054/abstract" class="button paperButton freePaperButton">FHN &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Beal DS, Ghosh SS, Guenther FH, Perkell JS. (2014). Impaired timing adjustments in response to time-varying auditory perturbation during connected speech production in persons who stutter. Brain Lang. 129:24-29.
                                    <div  style="float:right;">
                                        <a href="http://www.sciencedirect.com/science/article/pii/S0093934X14000042" class="button paperButton">B&L &#128279;</a>
                                        <a href="./pub/STUT_AP_Paper2_BLSC_rev2.3b_pub.pdf" class="button paperButton localPaperButton">Author Manuscript</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Beal DS, Ghosh SS, Tiede MK, Guenther FH, Perkell JS. (2012). Weak responses to auditory feedback perturbation during articulation in persons who stutter: Evidence for abnormal auditory-motor transformation. PLoS ONE. 7(7):e41830.
                                    <div  style="float:right;">
                                        <a href="http://www.plosone.org/article/info:doi/10.1371/journal.pone.0041830" class="button paperButton freePaperButton">PLoS ONE &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3402433/" class="button paperButton freePaperButton">PMC Manuscript &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Ghosh SS, Guenther FH, Perkell JS. (2011). Focal manipulations of formant trajectories reveal a role of auditory feedback in the online control of both within-syllable and between-syllable speech timing. J. Neurosci. 31(45):16483-16490.
                                    <div  style="float:right;">
                                        <a href="http://www.jneurosci.org/content/31/45/16483" class="button paperButton freePaperButton">JoN &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3268045/" class="button paperButton freePaperButton">PMC Manuscript &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Ghosh SS, Guenther FH, Perkell JS. (2010). Adaptive auditory feedback control of the production of the formant trajectories in the Mandarin triphthong /iau/ and its patterns of generalization. J. Acoust. Soc. Am. 128(4):2033-2048.
                                    <div  style="float:right;">
                                        <a href="http://scitation.aip.org/content/asa/journal/jasa/128/4/10.1121/1.3479539" class="button paperButton">JASA &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2981117/" class="button paperButton freePaperButton">PMC Manuscript &#128279;</a>
                                    </div>
                                </li>

                            </ul>
                        </p>

                        <p><h4>Theses and Dissertations</h4></p>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    <u>Cai S</u> (2012). Online Control of Articulation Based on Auditory Feedback in Normal Speech and Stuttering: Behavioral and Modeling Studies. Ph.D. dissertation, Harvard-MIT Division of Health Science and Technology, Massachusetts Institute of Technology, Cambridge, MA, USA.
                                    <div  style="float:right;">
                                        <a href="./pub/Cai_PhD_Thesis_2012_MIT_Final.pdf" class="button paperButton localPaperButton">Local PDF</a>
                                        <a href="http://dspace.mit.edu/handle/1721.1/70812" class="button paperButton freePaperButton">MIT DSpace &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u> (2012). Adaptive auditory-motor control of the time-varying formant trajectories in vowels and its patterns of generalization. Master's thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA.
                                    <div  style="float:right;">
                                        <a href="http://dspace.mit.edu/handle/1721.1/70789" class="button paperButton freePaperButton">MIT DSpace &#128279;</a>
                                    </div>
                                </li>
                            </ul>
                        </p>

                        <p><h4>Conference papers and presentations</h4></p>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    Van Brenk F, Terband H, <u>Cai S</u> (2014) Auditory feedback perturbation in adults and children. 2014 Motor Speech Conference, Sarasota, FL, USA, Feb. 27 - Mar. 2, 2014.
                                    <div  style="float:right;">
                                        <a href="http://www.madonna.org/file_download/b5a18a78-e45c-49b0-8c10-04caf4fa7809" class="button paperButton freePaperButton">Abstract &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Terband H, van Brenk H, van der Zee A, Nijssen M, <u>Cai S</u> (2014) Auditory feedback perturbation in children with developmental speech sound disorders. 2014 Motor Speech Conference, Sarasota, FL, USA, Feb. 27 - Mar. 2, 2014.
                                    <div  style="float:right;">
                                        <a href="http://www.madonna.org/file_download/10d4b905-90ec-487e-97ef-241e6d8bb059" class="button paperButton freePaperButton">Abstract &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Guenther FH (2013) Local time-warping in auditory feedback alters articulatory timing in connected multisyllabic speech containing vowels, fricatives and stops. Presented at
                                    <a href="http://speechneuro.ucsf.edu/events/asa-satellite-symposium-neural-bases-speech-production">Acoustical Soceity of America (ASA) Satellite Symposium: Neural Bases of Speech Production</a>, San Francisco, CA, USA, December 1, 2013.
                                    <div  style="float:right;">
                                        <a href="./pub/ASA-Symposium-UCSF-2013_pres.pdf" class="button paperButton localPaperButton">Slides (Local PDF)</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Bunnell HT, Patel R. (2013). Unsupervised vocal-tract length estimation through model-based acoustic-to-articulatory inversion. To be presented at 14th Annual Conference of the International Speech Communication Association (InterSpeech 2013), Lyon, France, Aug. 25-29, 2013.
                                    <div  style="float:right;">
                                        <a href="http://localhost:63342/www3/old-1/pub/VocaliD_VTLE_IS2013.pdf" class="button paperButton localPaperButton">Local PDF</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Wieland EA, <u>Cai S</u>, Ayres K, Chang S-E. (2013) Speech motor adaptation to auditory perturbation in children who do and do not stutter. 2013 Michigan Speech and Hearing Assoc. Annual Conference, Dearborn, Michigan, USA, March 21-23, 2013.
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Beal DS, Guenther FH, Perkell JS, Ghosh SS. (2012) FMRI resting state connectivity of the brain in stuttering. Society for Neuroscience (SfN) Annual Meeting 2012, New Orleans, LA, Oct. 13 – 17, 2012.
                                    <div  style="float:right;">
                                        <a href="./pub/CaiEtAl_SfN-Poster-2012.pdf" class="button paperButton localPaperButton">Poster (Local PDF)</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Beal DS, Segawa J, Tourville JA, <u>Cai S</u>, Guenther FH. (2012) Speech motor sequence learning difficulties in persistent developmental stuttering: An fMRI study. Society for Neuroscience (SfN) Annual Meeting 2012, New Orleans, LA, Oct. 13 – 17, 2012.
                                </li>

                                <li class="paperListItem">
                                    Schaefer M, McAuliffe MCM, Liss JM, Katseff S, O'Beirne GA, <u>Cai S</u>. (2012). Responses to manipulations in auditory feedback: The effect of aging. 2012 Motor Speech Conference, Santa Rosa, CA, USA. Feb. 29 – March 4, 2012.
                                </li>

                                <li class="paperListItem">
                                    Beal DS, <u>Cai S</u>, Guenther FH, Ghosh SS, Tiede MK, Perkell, JS. (2012). The relations among stuttering severity, experiences, and kinematic variability measures. 2012 Motor Speech Conference, Santa Rosa, CA, USA. Feb. 29 – March 4, 2012.
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Beal DS, Ghosh SS, Tiede MK, Guenther FH, Perkell JS (2011). Comparing auditory-motor interaction in static and time-varying articulation between stutterers and normal speakers. The 3rd Neurobiology of Language Conference (NLC), Annapolis, MD, Nov. 10 – 11, 2011.
                                    <div  style="float:right;">
                                        <a href="./pub/CaiEtAl_StuttteringAuditoryPerturb_NLC2011.pdf" class="button paperButton localPaperButton">Poster (Local PDF)</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Beal DS, Tiede MK, Perkell JS, Guenther FH, Ghosh SS. (2011). Relating the kinematic variability of speech to MRI-based structural integrity of brain white matter in people who stutter and people with fluent speech. Society for Neuroscience (SfN) Annual Meeting 2011, Washington, DC, Nov. 12 – 16, 2011.
                                </li>

                                <li class="paperListItem">
                                    Beal DS, <u>Cai S</u>, Ghosh SS, Tiede MK, Perkell, JS. (2011). The Relations Among Stuttering Severity, Experiences, & Kinematic Variability Measures. American Speech, Language and Hearing Association (ASHA) Annual Convention, San Diego, CA, Nov. 17-19, 2011.
                                </li>

                                <li class="paperListItem">
                                    Beal DS, Tourville JA, <u>Cai S</u>, Segawa J, Guenther FH. (2011). An fMRI Study of Speech-Sequence Learning in People Who Stutter. American Speech, Language and Hearing Association (ASHA) Annual Convention, San Diego, CA, Nov. 17-19, 2011.
                                </li>

                                <li class="paperListItem">
                                    Schaefer M, McAuliffe MCM, Liss JM, O’Beirne GA, <u>Cai S</u>. (2011). Responses of older individuals to manipulations in auditory feedback: Preliminary findings. The 8th Asia Pacific Conference on Speech, Language and Hearing, Christchurch, Canterbury, New Zealand, Jan. 11-14, 2011.
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Ghosh SS, Guenther FH, Perkell JS. (2010). The role of auditory feedback in the online control of multisyllabic articulation. International Summer School on Cognitive and Physical Models of Speech Production, Speech Perception and Production-Perception Interaction 2010, Berlin, Germany, Sept. 21 – Oct. 1, 2010.
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Ghosh SS, Guenther FH, and Perkell JS. (2010). Coordination of the first and second formants of the Mandarin triphthong /iau/ revealed by adaptation to auditory perturbations. (Abstract) J. Acoust. Soc. Am. 127(3), 2018. The 159th Meeting of the Acoustical Society of America, Baltimore, MD, April 19 - 23, 2010.
                                    <div  style="float:right;">
                                        <a href="./pub/ASA2010_SCai_5aSC8.pdf" class="button paperButton localPaperButton">Poster (Local PDF)</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Ghosh SS, Perkell JS, Guenther FH. (2010). The role of auditory feedback in the online control of articulatory trajectories and timing in a multi-syllabic utterance. 2010 Motor Speech Conference, Savannah, GA, March 4 - 7, 2010
                                    <div  style="float:right;">
                                        <a href="./pub/msc2009_paper1_final_www.pdf" class="button paperButton localPaperButton">Poster (Local PDF)</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Boucek M, Ghosh SS, Guenther FH, Perkell JS. (2008). A system for online dynamic perturbation of formant frequencies and results from perturbation of the Mandarin triphthong /iau/. In Proceedings of the 8th Intl. Seminar on Speech Production, Strasbourg, France, Dec. 8 - 12, 2008. pp. 65-68.
                                    <div  style="float:right;">
                                        <a href="http://issp2008.loria.fr/Proceedings/PDF/issp2008-10.pdf" class="button paperButton freePaperButton">ISSP'08 &#128279;</a>
                                    </div>
                                </li>

                            </ul>


                        </p>


                        <h3 id="papers_audition">&#167; Auditory neurophysiology</h3>

                        <p><h4>Journal papers</h4></p>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    <u>Cai S</u>, Ma W-L, Young ED. (2009). Encoding intensity in ventral cochlear nucleus neurons following acoustic trauma: implications for loudness recruitment. J. Assoc. Res. Otolaryngol. 10(1):5-22.
                                    <div  style="float:right;">
                                        <a href="http://link.springer.com/article/10.1007%2Fs10162-008-0142-y" class="button paperButton">JARO &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2644387/" class="button paperButton freePaperButton">JARO Commentary &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2644394/" class="button paperButton freePaperButton">PMC Manuscript &#128279;</a>
                                    </div>
                                </li>
                            </ul>
                        </p>

                        <p><h4>Theses and Dissertations</h4></p>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    <u>Cai S</u> (2007). Intensity encoding of ventral cochlear nucleus neurons in normal and deafened cats and correlates of loudness recruitment. Master's thesis, Department of Biomedical Engineering, The Johns Hopkins University, Baltimore, MD, USA
                                    <div  style="float:right;">
                                        <a href="./pub/CaiS2007_MSThesis_JHU.pdf" class="button paperButton localPaperButton">Local PDF</a>
                                        <a href="https://catalyst.library.jhu.edu/catalog/bib_2881290" class="button paperButton">JHU Library &#128279;</a>
                                    </div>
                                </li>
                            </ul>
                        </p>

                        <p><h4>Conference papers and presentations</h4></p>


                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    <u>Cai S</u>, Ma W-L, Letham B, Young ED. (2007). Rate-intensity functions of ventral cochlear nucleus in normal and hearing-impaired cats and their possible relationships to loudness recruitment. 30th Assoc. Res. Otolaryngol. Midwinter Meeting, Denver, Colorado, Feb. 10 - 15, 2007.
                                </li>

                                <li class="paperListItem">
                                    Letham B, Ma W-L, <u>Cai S</u>, Young ED. (2007). Acoustic trauma induces long-term temporal correlations in DCN. 30th Assoc. Res. Otolaryngol. Midwinter Meeting, Denver, Colorado, Feb. 10 - 15, 2007.
                                </li>
                            </ul>
                        </p>


                        <h3 id="papers_visual_protheses">&#167; Bionic eyes</h3>

                        <p>
                            <ul class="paperList">
                                <li class="paperListItem">
                                    Fu L, <u>Cai S</u>, Zhang H, Hu G, Zhang X. (2006). Psychophysics of reading with a limited number of pixels: Towards the rehabilitation of reading ability with visual prosthesis. Vision Res. 46:1292-1301.
                                    <div  style="float:right;">
                                        <a href="http://linkinghub.elsevier.com/retrieve/pii/S0042698905006012" class="button paperButton freePaperButton">Vision Res. &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pubmed/16380147" class="button paperButton freePaperButton">PubMed &#128279;</a>

                                    </div>
                                </li>

                                <li class="paperListItem">
                                    Fu L, Zhang H, <u>Cai S</u>, Hu G. (2006). Chinese printed text reading performance with pixelized prosthetic vision system. (in Chinese), J. Tsinghua Univ. (Sci. & Tech.) 46(6): 858-860, 871.
                                    <div  style="float:right;">
                                        <a href="http://d.wanfangdata.com.cn/periodical_qhdxxb200606028.aspx" class="button paperButton">WanFang Data &#128279;</a>
                                    </div>
                                </li>

                                <li class="paperListItem">
                                    <u>Cai S</u>, Fu L, Zhang H, Hu G, Liang Z. (2005). Prosthetic visual acuity in irregular phosphene arrays under two down-sampling schemes: a simulation study. In Proc. 27th Annual Intl. Conf. IEEE-EMBS, Shanghai, China, Sept. 1 - 4, 2005. Vol. 5, pp. 5523-5526.
                                    <div  style="float:right;">
                                        <a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1615656" class="button paperButton">IEEE &#128279;</a>
                                        <a href="http://www.ncbi.nlm.nih.gov/pubmed/17281426" class="button paperButton">PubMed &#128279;</a>
                                    </div>
                                </li>

                            </ul>
                        </p>

                        </body>

                    </div>
                </section>

				<!-- About Me -->
                <section id="about" class="three">
                    <div class="container">

                        <header>
                            <h2>About</h2>
                        </header>

                        <body>
                            <h3>Pronuncation of the name</h3>
                            <ul class="schoolList">
                                <li>
                                        The <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet" target="_blank">IPA</a> of "Shanqing" is /
                                        <a rel="group" href="https://en.wikipedia.org/wiki/Voiceless_retroflex_sibilant" target="_blank">ʂ</a>
                                        an&nbsp;
                                        <a rel="group" href="https://en.wikipedia.org/wiki/Voiceless_alveolo-palatal_affricate" target="_blank">tɕ</a>
                                        <a rel="group" href="https://en.wikipedia.org/wiki/Aspirated_consonant" target="_blank">ʰ</a>
                                        i
                                        <a rel="group" href="https://en.wikipedia.org/wiki/Velar_nasal" target="_blank">ŋ</a>
                                        /.
                                        The spelling is based on the transliteration system of <a href="https://en.wikipedia.org/wiki/Pinyin" target="_blank">Pinyin</a>.

                                </li>
                                <li>
                                    In American English, the approximation would be "Shahn-Cheeng".
                                </li>
                            </ul>

                            <h3>Educational Background</h3>

                            <ul class="schoolList">
                                <li>
                                    <a href="http://www.hms.harvard.edu/dms/shbt/">SHBT</a>,
                                    <a href="http://hst.mit.edu/">Harvard-MIT Division of Health Science and Technology</a><br/>
                                    Ph.D. in Speech and Hearing Bioscience and Technology, 2012
                                </li>

                                <li>
                                    <a href="http://eecsweb.mit.edu/">EECS</a>,
                                    <a href="http://web.mit.edu/">Massachusetts Institute of Technology</a><br/>
                                    M.S. in Electrical Engineering and Computer Science, 2012
                                </li>

                                <li>
                                    <a href="http://www.bme.jhu.edu/">BME</a>,
                                    <a href="http://www.jhu.edu/">The Johns Hopkins University</a><br/>
                                    M.S.E. in Biomedical Engineering, 2007
                                </li>

                                <li>
                                    <a href="http://bme.med.tsinghua.edu.cn/">BME</a>,
                                    <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a><br/>
                                    B.E. in Biomedical Engineering, 2005

                                </li>

                            </ul>

                            <a href="http://neurotree.org/neurotree/tree.php?pid=17653" class="button">My NeuroTree &#128279;</a>

                        </body>


                        <p></p>

                    </div>
                </section>


			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; 2015-2019 Shanqing Cai. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>

            <link rel="stylesheet" href="fancybox/source/jquery.fancybox.css?v=2.1.5" type="text/css" media="screen" />
            <script type="text/javascript" src="fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

            <script type="text/javascript">
                $(document).ready(function() {
                    $(".fancybox").fancybox();

                    if ( $(document).width() < 600 ) {
                    }
                });
            </script>

	</body>
</html>
